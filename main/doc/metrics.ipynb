{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils_2 as utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "from mrcnn.model import log\n",
    "\n",
    "from doc import train\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Path to Ballon trained weights\n",
    "# You can download this file from the Releases page\n",
    "# https://github.com/matterport/Mask_RCNN/releases\n",
    "BALLON_WEIGHTS_PATH = \"./pretrained_model_indiscapes.h5\"  # TODO: update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = train.Config()\n",
    "DOC_DIR = os.path.join(ROOT_DIR, \"datasets/doc/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class InferenceConfig(config.__class__):\n",
    "# \tGPU_COUNT = 1\n",
    "# \tIMAGES_PER_GPU = 1\n",
    "# \tIMAGE_RESIZE_MODE = \"square\"\n",
    "# \tDETECTION_MIN_CONFIDENCE = 0.6\n",
    "# \tDETECTION_NMS_THRESHOLD = 0.3\n",
    "# \tPRE_NMS_LIMIT = 12000\n",
    "# \tRPN_ANCHOR_SCALES = (8,32,64,256,1024)\n",
    "# \tRPN_ANCHOR_RATIOS = [1,3,10]\n",
    "\n",
    "# \tPOST_NMS_ROIS_INFERENCE = 12000\n",
    "    \n",
    "# config = InferenceConfig()\n",
    "# config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        150\n",
      "DETECTION_MIN_CONFIDENCE       0.5\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                23\n",
      "IMAGE_MIN_DIM                  256\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.0001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 2.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 2.0, 'mrcnn_mask_loss': 5.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               150\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           object\n",
      "NUM_CLASSES                    11\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         3000\n",
      "PRE_NMS_LIMIT                  4000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [1, 3, 10]\n",
      "RPN_ANCHOR_SCALES              (8, 32, 64, 256, 1024)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.6\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    4096\n",
      "STEPS_PER_EPOCH                500\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           512\n",
      "USE_MINI_MASK                  False\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               80\n",
      "WEIGHT_DECAY                   0.001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Override the training configurations with a few\n",
    "# changes for inferencing.\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    IMAGE_RESIZE_MODE = \"square\"\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5\n",
    "    DETECTION_NMS_THRESHOLD = 0.3\n",
    "    PRE_NMS_LIMIT = 4000\n",
    "\n",
    "    # ROIs kept after non-maximum suppression (training and inference)\n",
    "    POST_NMS_ROIS_INFERENCE = 1000\n",
    "    \n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Count: 53\n",
      "Class Count: 11\n",
      "  0. BG                                                \n",
      "  1. H-V                                               \n",
      "  2. H                                                 \n",
      "  3. CLS                                               \n",
      "  4. PD                                                \n",
      "  5. PB                                                \n",
      "  6. CC                                                \n",
      "  7. P                                                 \n",
      "  8. D                                                 \n",
      "  9. LM                                                \n",
      " 10. BL                                                \n"
     ]
    }
   ],
   "source": [
    "# Load validation dataset\n",
    "dataset = train.Dataset()\n",
    "dataset.load_data(DOC_DIR, \"val\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "print(\"Image Count: {}\".format(len(dataset.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset.num_classes))\n",
    "for i, info in enumerate(dataset.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 1 but is rank 3 for 'roi_align_classifier_1/CropAndResize' (op: 'CropAndResize') with input shapes: [?,?,?,256], [?,4], [?], [2,1,4].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1658\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 1 but is rank 3 for 'roi_align_classifier_1/CropAndResize' (op: 'CropAndResize') with input shapes: [?,?,?,256], [?,4], [?], [2,1,4].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9a6b04ba9156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n\u001b[0;32m----> 4\u001b[0;31m                               config=config)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/project/Final_MaskRCNN/Instance-segmentation-master_copy/mrcnn/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_log_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/project/Final_MaskRCNN/Instance-segmentation-master_copy/mrcnn/model.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, mode, config)\u001b[0m\n\u001b[1;32m   2116\u001b[0m                                      \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOOL_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m                                      \u001b[0mtrain_bn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN_BN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2118\u001b[0;31m                                      fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)\n\u001b[0m\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2120\u001b[0m             \u001b[0;31m# Detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/project/Final_MaskRCNN/Instance-segmentation-master_copy/mrcnn/model.py\u001b[0m in \u001b[0;36mfpn_classifier_graph\u001b[0;34m(rois, feature_maps, image_meta, pool_size, num_classes, train_bn, fc_layers_size)\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrois\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrois\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     x = PyramidROIAlign([w, h],\n\u001b[0;32m--> 949\u001b[0;31m                         name=\"roi_align_classifier\")([rois, image_meta] + feature_maps)\n\u001b[0m\u001b[1;32m    950\u001b[0m     \u001b[0;31m# Two 1024 FC layers (implemented with Conv2D for consistency)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\"valid\"),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/project/Final_MaskRCNN/Instance-segmentation-master_copy/mrcnn/model.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    441\u001b[0m             pooled.append(tf.image.crop_and_resize(\n\u001b[1;32m    442\u001b[0m                 \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m                 method=\"bilinear\"))\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;31m# Pack pooled features into one tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_image_ops.py\u001b[0m in \u001b[0;36mcrop_and_resize\u001b[0;34m(image, boxes, box_ind, crop_size, method, extrapolation_value, name)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;34m\"CropAndResize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_ind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbox_ind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                          \u001b[0mcrop_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrop_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                          extrapolation_value=extrapolation_value, name=name)\n\u001b[0m\u001b[1;32m    391\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3300\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1823\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 1 but is rank 3 for 'roi_align_classifier_1/CropAndResize' (op: 'CropAndResize') with input shapes: [?,?,?,256], [?,4], [?], [2,1,4]."
     ]
    }
   ],
   "source": [
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to balloon weights file\n",
    "\n",
    "# Download file from the Releases page and set its path\n",
    "# https://github.com/matterport/Mask_RCNN/releases\n",
    "# weights_path = \"/path/to/mask_rcnn_balloon.h5\"\n",
    "\n",
    "# Or, load the last model you trained\n",
    "# weights_path = model.find_last()\n",
    "weights_path=BALLON_WEIGHTS_PATH\n",
    "# Load weights\n",
    "print(\"Loading weights \", weights_path)\n",
    "#model.load_weights(weights_path, by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "model.load_weights(weights_path, by_name=True,exclude=[\"mrcnn_bbox_fc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dic={}\n",
    "all_images_test=dataset.image_ids\n",
    "cnt=0\n",
    "\n",
    "\n",
    "avg_pagewise=[]\n",
    "avg_classwise=[]\n",
    "acc_classwise=[]\n",
    "avg_fIOU=0.0\n",
    "avg_mAP=0.0\n",
    "avg_p=0.0\n",
    "avg_r=0.0\n",
    "avg_pres=[]\n",
    "avg_rec=[]\n",
    "avg_mAP_range=[]\n",
    "\n",
    "for ind in range(len(all_images_test)):\n",
    "    cnt+=1\n",
    "    image_id=all_images_test[ind]\n",
    "    print(ind,\" : \",image_id)\n",
    "    #image_id=34\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =    modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "    info = dataset.image_info[image_id]\n",
    "    # print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n",
    "    #                                        dataset.image_reference(image_id)))\n",
    "    img_name=info['id']\n",
    "    print(img_name)\n",
    "    # Run object detection\n",
    "    # print(image.shape)\n",
    "    # print(image)\n",
    "    # image=cv2.imread('efeo_010_01_03.jpg',1)\n",
    "    # image,_,_,_,_=utils.resize_image(image,min_dim=256, max_dim=1024)\n",
    "    #print(image.shape)\n",
    "    results = model.detect([image], verbose=0)\n",
    "    #print(results)\n",
    "    # Display results\n",
    "    #ax = get_ax(1)\n",
    "    r = results[0]\n",
    "    #print(r)\n",
    "    # ccc=visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "    #                             dataset.class_names, r['scores'], ax=ax,\n",
    "    #                             title=\"Predictions\",show_bbox=False,show_mask=True)\n",
    "    # visualize.display_instances(image, gt_bbox, gt_mask, gt_class_id, \n",
    "    #                             dataset.class_names, ax=get_ax(1),\n",
    "    #                             show_bbox=False, show_mask=False,\n",
    "    #                             title=\"Ground Truth\")\n",
    "    # visualize.display_instances(image, gt_bbox, gt_mask, gt_class_id, \n",
    "    #                             dataset.class_names, ax=get_ax(1),\n",
    "    #                             show_bbox=False, show_mask=True,\n",
    "    #                             title=\"Ground Truth\")\n",
    "\n",
    "    # print(\"gt_bbox : \",gt_bbox.shape)\n",
    "    # print(\"gt_class_id : \",gt_class_id)\n",
    "    # print(\"gt_mask : \",gt_mask.shape)\n",
    "    # print(\"scores: \",r['scores'].shape )\n",
    "    # print(\"gt_bbox : \",r['rois'].shape)\n",
    "    # print(\"gt_class_id : \",r['class_ids'])\n",
    "    # print(\"gt_mask : \",r['masks'].shape)\n",
    "    if len(r['rois'])>0:\n",
    "        pagewise,weighted,com_freq,mAP_out,pres_out,rec_out,mAP_range,class_wise,class_acc=utils.compute_per_region_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                                r['rois'], r['class_ids'], r['scores'], r['masks'],iou_threshold=0.0,score_threshold=0.0)\n",
    "        # # res1,_,_,_=utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "        # #                        r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
    "        # # print(res)\n",
    "        # # print(res1)\n",
    "        # visualize.display_differences(\n",
    "        #     image,\n",
    "        #     gt_bbox, gt_class_id, gt_mask,\n",
    "        #     r['rois'], r['class_ids'], r['scores'], r['masks'],\n",
    "        #     dataset.class_names, ax=get_ax(),\n",
    "        #     show_box=False, show_mask=False,\n",
    "        #     iou_threshold=0.5, score_threshold=0.5)\n",
    "        avg_classwise.append(list(class_wise.values()))\n",
    "        acc_classwise.append(list(class_acc.values()))\n",
    "        #print(avg_classwise)\n",
    "        tt_dic={}\n",
    "        pres_out=np.array(pres_out)\n",
    "        rec_out=np.array(rec_out)   \n",
    "        p=pres_out.mean()\n",
    "        r=rec_out.mean()\n",
    "        print(\"pagewise : \",pagewise)\n",
    "        print(\"f-weighted :\",weighted)\n",
    "        #print(com_freq)\n",
    "        print(\"mean AP : \",mAP_out)\n",
    "        print(\"mean precision : \",p)\n",
    "        print(\"mean recall : \",r)\n",
    "        print(\"prec_range : \",pres_out)\n",
    "        print(\"rec_range : \",rec_out)\n",
    "        print(\"AP_range : \",mAP_range)\n",
    "        avg_pagewise.append(list(pagewise.values()))\n",
    "        avg_fIOU+=weighted\n",
    "        avg_mAP+=mAP_out\n",
    "        avg_p+=p\n",
    "        avg_r+=r\n",
    "        avg_pres.append(pres_out)\n",
    "        avg_rec.append(rec_out)\n",
    "        avg_mAP_range.append(mAP_range)\n",
    "\n",
    "\n",
    "\n",
    "        tt_dic['pagewise']=pagewise\n",
    "        tt_dic['f-weighted']=weighted\n",
    "        tt_dic['mAP']=mAP_out\n",
    "        tt_dic['mprec']=p\n",
    "        tt_dic['mrec']=r\n",
    "        tt_dic['Precision']=pres_out\n",
    "        tt_dic['recall']=rec_out\n",
    "        tt_dic['maP_range']=mAP_range\n",
    "\n",
    "        # print(res[2][1])\n",
    "        # print(r['masks'].shape,gt_mask.shape)\n",
    "        # res2=utils.compute_overlaps_masks(gt_mask,r['masks'])\n",
    "        # print(np.array(res2).shape)\n",
    "        # log(\"gt_class_id\", gt_class_id)\n",
    "        # log(\"gt_bbox\", gt_bbox)\n",
    "        # log(\"gt_mask\", gt_mask)\n",
    "        out_dic[img_name]=tt_dic\n",
    "\n",
    "    # In[ ]:\n",
    "dic_freq_bhoomi={1:1.0,2:46.0,3:159.0,4:305.0,5:28.0,6:26.0,7:1.0,8:1.0,9:14.0,10:14.0}\n",
    "dic_freq_PIH={1:1.0,2:2.0,3:372.0,4:6.0,5:53.0,6:73.0,7:7.0,8:13.0,9:8.0,10:52.0}\n",
    "\n",
    "dic_freq_all={1:1.0,2:48.0,3:531,4:311,5:81,6:99,7:8.0,8:14.0,9:22.0,10:66.0}\n",
    "# arr_freq=list(dic_freq.values())\n",
    "\n",
    "avg_classwise=np.sum(avg_classwise,axis=0)\n",
    "acc_classwise=np.sum(acc_classwise,axis=0)\n",
    "avg_fIOU=avg_fIOU*1.0/cnt\n",
    "avg_mAP=avg_mAP*1.0/cnt\n",
    "avg_p=avg_p*1.0/cnt\n",
    "avg_r=avg_r*1.0/cnt\n",
    "avg_pres=np.mean(np.array(avg_pres),axis=0)\n",
    "avg_rec=np.mean(np.array(avg_rec),axis=0)\n",
    "avg_pagewise=np.mean(np.array(avg_pagewise),axis=0)\n",
    "avg_mAP_range=np.mean(np.array(avg_mAP_range),axis=0)\n",
    "\n",
    "class_weighted=0.0\n",
    "for i in range(len(avg_classwise)):\n",
    "    avg_classwise[i]=(avg_classwise[i]*1.0)/dic_freq_PIH[i+1]\n",
    "    acc_classwise[i]=(acc_classwise[i]*1.0)/dic_freq_PIH[i+1]\n",
    "\n",
    "\n",
    "print(\"final results: /////########################\")\n",
    "\n",
    "print(\"avg_IOU_classwise : \",avg_classwise)\n",
    "print(\"acc_classwise : \",acc_classwise)\n",
    "print(\"avg_fIOU : \",avg_fIOU)\n",
    "print(\"avg_pagewise : \",avg_pagewise)\n",
    "print(\"avg_mAP : \",avg_mAP)\n",
    "print(\"avg prec : \",avg_p)\n",
    "print(\"avg rec : \",avg_r)\n",
    "print(\"avg_mAP_range : \",avg_mAP_range)\n",
    "print(\"avg_pres_range : \",avg_pres)\n",
    "print(\"avg_rec_range : \",avg_rec)\n",
    "\n",
    "\n",
    "with open('pih_metrics.pickle','wb') as f:\n",
    "    pickle.dump(out_dic,f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dic={}\n",
    "all_images_test=dataset.image_ids\n",
    "cnt=0\n",
    "avg_pagewise=[]\n",
    "avg_classwise=[]\n",
    "acc_classwise=[]\n",
    "avg_fIOU=0.0\n",
    "avg_mAP=0.0\n",
    "avg_p=0.0\n",
    "avg_r=0.0\n",
    "avg_pres=[]\n",
    "avg_rec=[]\n",
    "avg_mAP_range=[]\n",
    "\n",
    "for ind in range(len(all_images_test)):\n",
    "    cnt+=1\n",
    "    image_id=all_images_test[ind]\n",
    "    print(ind,\" : \",image_id)\n",
    "    #image_id=34\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =    modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "    info = dataset.image_info[image_id]\n",
    "    # print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n",
    "    #                                        dataset.image_reference(image_id)))\n",
    "    img_name=info['id']\n",
    "    print(img_name)\n",
    "    # Run object detection\n",
    "    # print(image.shape)\n",
    "    # print(image)\n",
    "    # image=cv2.imread('efeo_010_01_03.jpg',1)\n",
    "    # image,_,_,_,_=utils.resize_image(image,min_dim=256, max_dim=1024)\n",
    "    #print(image.shape)\n",
    "    results = model.detect([image], verbose=0)\n",
    "    #print(results)\n",
    "    # Display results\n",
    "    #ax = get_ax(1)\n",
    "    r = results[0]\n",
    "    #print(r)\n",
    "    # ccc=visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "    #                             dataset.class_names, r['scores'], ax=ax,\n",
    "    #                             title=\"Predictions\",show_bbox=False,show_mask=True)\n",
    "    # visualize.display_instances(image, gt_bbox, gt_mask, gt_class_id, \n",
    "    #                             dataset.class_names, ax=get_ax(1),\n",
    "    #                             show_bbox=False, show_mask=False,\n",
    "    #                             title=\"Ground Truth\")\n",
    "    # visualize.display_instances(image, gt_bbox, gt_mask, gt_class_id, \n",
    "    #                             dataset.class_names, ax=get_ax(1),\n",
    "    #                             show_bbox=False, show_mask=True,\n",
    "    #                             title=\"Ground Truth\")\n",
    "\n",
    "    # print(\"gt_bbox : \",gt_bbox.shape)\n",
    "    # print(\"gt_class_id : \",gt_class_id)\n",
    "    # print(\"gt_mask : \",gt_mask.shape)\n",
    "    # print(\"scores: \",r['scores'].shape )\n",
    "    # print(\"gt_bbox : \",r['rois'].shape)\n",
    "    # print(\"gt_class_id : \",r['class_ids'])\n",
    "    # print(\"gt_mask : \",r['masks'].shape)\n",
    "    pagewise,weighted,com_freq,mAP_out,pres_out,rec_out,mAP_range,class_wise,class_acc=utils.compute_per_region_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                        r['rois'], r['class_ids'], r['scores'], r['masks'],iou_threshold=0.4,score_threshold=0.4)\n",
    "# # res1,_,_,_=utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "# #                        r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
    "# # print(res)\n",
    "# # print(res1)\n",
    "# visualize.display_differences(\n",
    "#     image,\n",
    "#     gt_bbox, gt_class_id, gt_mask,\n",
    "#     r['rois'], r['class_ids'], r['scores'], r['masks'],\n",
    "#     dataset.class_names, ax=get_ax(),\n",
    "#     show_box=False, show_mask=False,\n",
    "#     iou_threshold=0.5, score_threshold=0.5)\n",
    "\n",
    "#         cnt+=1\n",
    "    avg_classwise.append(list(class_wise.values()))\n",
    "    acc_classwise.append(list(class_acc.values()))\n",
    "    #print(avg_classwise)\n",
    "    tt_dic={}\n",
    "    pres_out=np.array(pres_out)\n",
    "    rec_out=np.array(rec_out)   \n",
    "    p=pres_out.mean()\n",
    "    r=rec_out.mean()\n",
    "#     print(\"pagewise : \",pagewise)\n",
    "#     print(\"f-weighted :\",weighted)\n",
    "#     #print(com_freq)\n",
    "#     print(\"mean AP : \",mAP_out)\n",
    "#     print(\"mean precision : \",p)\n",
    "#     print(\"mean recall : \",r)\n",
    "#     print(\"prec_range : \",pres_out)\n",
    "#     print(\"rec_range : \",rec_out)\n",
    "#     print(\"AP_range : \",mAP_range)\n",
    "    avg_pagewise.append(list(pagewise.values()))\n",
    "    avg_fIOU+=weighted\n",
    "    avg_mAP+=mAP_out\n",
    "    avg_p+=p\n",
    "    avg_r+=r\n",
    "    avg_pres.append(pres_out)\n",
    "    avg_rec.append(rec_out)\n",
    "    avg_mAP_range.append(mAP_range)\n",
    "\n",
    "    tt_dic['pagewise']=pagewise\n",
    "    tt_dic['f-weighted']=weighted\n",
    "    tt_dic['mAP']=mAP_out\n",
    "    tt_dic['mprec']=p\n",
    "    tt_dic['mrec']=r\n",
    "    tt_dic['Precision']=pres_out\n",
    "    tt_dic['recall']=rec_out\n",
    "    tt_dic['maP_range']=mAP_range\n",
    "\n",
    "# print(res[2][1])\n",
    "# print(r['masks'].shape,gt_mask.shape)\n",
    "# res2=utils.compute_overlaps_masks(gt_mask,r['masks'])\n",
    "# print(np.array(res2).shape)\n",
    "# log(\"gt_class_id\", gt_class_id)\n",
    "# log(\"gt_bbox\", gt_bbox)\n",
    "# log(\"gt_mask\", gt_mask)\n",
    "    out_dic[img_name]=tt_dic\n",
    "\n",
    "    # In[ ]:\n",
    "dic_freq_bhoomi={1:1.0,2:46.0,3:159.0,4:305.0,5:28.0,6:26.0,7:1.0,8:1.0,9:14.0,10:14.0}\n",
    "dic_freq_PIH={1:1.0,2:2.0,3:372.0,4:6.0,5:53.0,6:73.0,7:7.0,8:13.0,9:8.0,10:52.0}\n",
    "\n",
    "dic_freq_all={1:1.0,2:48.0,3:531,4:311,5:81,6:99,7:8.0,8:14.0,9:22.0,10:66.0}\n",
    "\n",
    "# dic_freq_bhoomi={1:1.0,2:1.0,3:159.0,4:305.0,5:28.0,6:26.0,7:1.0,8:1.0,9:14.0,10:14.0}\n",
    "# dic_freq_PIH={1:1.0,2:1.0,3:372.0,4:6.0,5:53.0,6:73.0,7:7.0,8:13.0,9:8.0,10:52.0}\n",
    "\n",
    "# dic_freq_all={1:1.0,2:1.0,3:531,4:311,5:81,6:99,7:8.0,8:14.0,9:22.0,10:66.0}\n",
    "\n",
    "# dic_freq_bhoomi={1:1.0,2:1.0}\n",
    "# dic_freq_PIH={1:1.0,2:1.0}\n",
    "# dic_freq_all={1:1.0,2:1.0}\n",
    "\n",
    "# arr_freq=list(dic_freq.values())\n",
    "\n",
    "avg_classwise=np.sum(avg_classwise,axis=0)\n",
    "acc_classwise=np.sum(acc_classwise,axis=0)\n",
    "avg_fIOU=avg_fIOU*1.0/cnt\n",
    "avg_mAP=avg_mAP*1.0/cnt\n",
    "avg_p=avg_p*1.0/cnt\n",
    "avg_r=avg_r*1.0/cnt\n",
    "avg_pres=np.mean(np.array(avg_pres),axis=0)\n",
    "avg_rec=np.mean(np.array(avg_rec),axis=0)\n",
    "avg_pagewise=np.mean(np.array(avg_pagewise),axis=0)\n",
    "avg_mAP_range=np.mean(np.array(avg_mAP_range),axis=0)\n",
    "\n",
    "class_weighted=0.0\n",
    "for i in range(len(avg_classwise)):\n",
    "    avg_classwise[i]=(avg_classwise[i]*1.0)/dic_freq_all[i+1]\n",
    "    acc_classwise[i]=(acc_classwise[i]*1.0)/dic_freq_all[i+1]\n",
    "\n",
    "\n",
    "print(\"final results: /////########################\")\n",
    "print(\"avg_IOU_classwise : \",avg_classwise)\n",
    "print(\"acc_classwise : \",acc_classwise)\n",
    "print(\"avg_fIOU : \",avg_fIOU)\n",
    "print(\"avg_pagewise : \",avg_pagewise)\n",
    "print(\"avg_mAP : \",avg_mAP)\n",
    "print(\"avg prec : \",avg_p)\n",
    "print(\"avg rec : \",avg_r)\n",
    "print(\"avg_mAP_range : \",avg_mAP_range)\n",
    "print(\"avg_pres_range : \",avg_pres)\n",
    "print(\"avg_rec_range : \",avg_rec)\n",
    "\n",
    "\n",
    "with open('pih_metrics.pickle','wb') as f:\n",
    "    pickle.dump(out_dic,f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_classwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#iou_threshold=0.4,score_threshold=0.4\n",
    "avg_IOU_classwise :  [('Hole(Virtual)', 0), ('Hole(Physical)', 0.7678649591965663), ('Character Line Segment', 0.5116226973857138), ('Physical Degradation', 0.0892081546979525), ('Page Boundary', 0.7404136104523582), ('Character Component', 0.432609309277567), ('Picture', 0.5850817594661933), ('Decorator', 0.034911221074079464), ('Library Marker', 0.3794140331304556), ('Boundary Line', 0.20353904331114614)]\n",
    "avg_acc_classwise :  [('Hole(Virtual)', 0), ('Hole(Physical)', 0.9994667309981126), ('Character Line Segment', 0.7429944756767025), ('Physical Degradation', 0.16640194344049966), ('Page Boundary', 0.886257289207145), ('Character Component', 0.6586898501071866), ('Picture', 0.8223311106363932), ('Decorator', 0.06186455488204956), ('Library Marker', 0.5513625144958496), ('Boundary Line', 0.3658131544930594)]\n",
    "class_count :  [('Hole(Virtual)', 0), ('Hole(Physical)', 26), ('Character Line Segment', 77), ('Physical Degradation', 24), ('Page Boundary', 73), ('Character Component', 36), ('Picture', 6), ('Decorator', 8), ('Library Marker', 19), ('Boundary Line', 20)]\n",
    "avg_fIOU :  0.606578642588633\n",
    "avg_mAP :  0.25803234347983095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_classwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fIOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
